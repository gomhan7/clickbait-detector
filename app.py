import streamlit as st
import joblib
import requests
from bs4 import BeautifulSoup
import re
import urllib.parse
from google.oauth2.service_account import Credentials
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime

# --- ì„¤ì • ---
# ëª¨ë¸ ë° ë²¡í„°ë¼ì´ì € íŒŒì¼ ê²½ë¡œ ì •ì˜ (ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”)
MODEL_PATH = "clickbait_model.pkl"
VEC_PATH = "tfidf_vectorizer.pkl"

# --- Streamlit í˜ì´ì§€ ì„¤ì • (ìŠ¤í¬ë¦½íŠ¸ì˜ ì²« ë²ˆì§¸ Streamlit ëª…ë ¹ì´ì–´ì•¼ í•¨!) ---
st.set_page_config(page_title="ë‚šì‹œì„± ë‰´ìŠ¤ íŒë³„ê¸°", page_icon="ğŸ£", layout="centered")


# --- ëª¨ë¸ ë° ë²¡í„°ë¼ì´ì € ë¡œë”© ---
@st.cache_resource
def load_model_and_vectorizer():
    """
    ì§€ì •ëœ ê²½ë¡œì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ê³¼ TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    try:
        model = joblib.load(MODEL_PATH)
        vectorizer = joblib.load(VEC_PATH)
        return model, vectorizer
    except FileNotFoundError:
        st.error(f"ì˜¤ë¥˜: í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:")
        st.error(f"- ëª¨ë¸ íŒŒì¼: `{MODEL_PATH}`")
        st.error(f"- ë²¡í„°ë¼ì´ì € íŒŒì¼: `{VEC_PATH}`")
        render_footer()
        st.stop()
        
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        render_footer()
        st.stop()

# ì•± ì‹œì‘ ì‹œ ëª¨ë¸ê³¼ ë²¡í„°ë¼ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
model, vectorizer = load_model_and_vectorizer()

# ì„œë²„ ë¡œê·¸ ê¸°ë¡
def log_to_google_sheets(method, input_text, result, score):
    # í•„ìš”í•œ scope ëª…ì‹œ
    scope = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive"
    ]

    # secrets.tomlì„ í†µí•œ ì¸ì¦
    creds = Credentials.from_service_account_info(
        st.secrets["google_sheets"], scopes=scope
    )
    client = gspread.authorize(creds)

    # ì‹œíŠ¸ ì—´ê¸°
    sheet = client.open("StreamlitLogs").sheet1

    # í˜„ì¬ ì‹œê°„ ê¸°ë¡
    timestamp = str(datetime.now())

    # ì‹œíŠ¸ì— ë¡œê·¸ ì¶”ê°€
    sheet.append_row([timestamp, method, input_text[:1000], result, f"{score}%"])


# í•˜ë‹¨ ì•ˆë‚´ë¥¼ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
def render_footer():
    st.markdown(
    """
    <hr style='margin-top: 20px; margin-bottom: 10px; border: none; height: 1px; background-color: #ccc;' />
    """,
    unsafe_allow_html=True
)
    
    st.markdown(
        """
        <div style='text-align: center; font-size: 0.9em; color: gray;'>
            ğŸ“ ë§Œì¡±ë„ ì¡°ì‚¬ : <a href='https://forms.gle/kn7hpCN1nixU4J599' target='_blank'>https://forms.gle/kn7hpCN1nixU4J599</a><br>
            ğŸ“§ ë¬¸ì˜ : JH.Moon213@gmail.com
        </div>
        """,
        unsafe_allow_html=True
    )
    st.markdown(
    """
    <div style='text-align: center'>
        <img src="https://i.imgur.com/VWthizR.png" width="40" />
    </div>
    """,
    unsafe_allow_html=True
    )
    
# --- ë‰´ìŠ¤ ë§í¬ì—ì„œ ì œëª©/ë³¸ë¬¸/ì¶œì²˜ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_info_from_url(url):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì œëª©, ë³¸ë¬¸, ì¶œì²˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        # --- ì¸ì½”ë”© ì²˜ë¦¬ ê°•í™” ---
        # 1. HTTP í—¤ë”ì˜ Content-Typeì—ì„œ charsetì„ í™•ì¸í•©ë‹ˆë‹¤.
        # 2. ë˜ëŠ” íŠ¹ì • ë„ë©”ì¸(dt.co.kr)ì— ëŒ€í•´ euc-krì„ ê°•ì œ ì ìš©í•©ë‹ˆë‹¤.
        
        # ê¸°ë³¸ ì¸ì½”ë”©ìœ¼ë¡œ ë¨¼ì € ì‹œë„
        text_content = response.text
        
        # Content-Type í—¤ë”ì—ì„œ charset ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜, dt.co.kr ë„ë©”ì¸ì´ë¼ë©´ euc-kr ê°•ì œ ì‹œë„
        content_type = response.headers.get('Content-Type', '').lower()
        if 'charset=euc-kr' in content_type or 'dt.co.kr' in url:
            try:
                # response.content (ë°”ì´íŠ¸)ë¥¼ euc-krë¡œ ë””ì½”ë”©
                text_content = response.content.decode('euc-kr', errors='replace') # errors='replace'ë¡œ ê¹¨ì§„ ë¬¸ì ëŒ€ì²´
                st.info("ğŸ’¡ ì¸ì½”ë”© ë¬¸ì œ ê°ì§€: `euc-kr`ë¡œ ì¬ë””ì½”ë”©í–ˆìŠµë‹ˆë‹¤.")
            except UnicodeDecodeError:
                st.warning("ê²½ê³ : `euc-kr` ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì§€ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")
                pass # euc-kr ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ì›ë˜ text_content ìœ ì§€

        soup = BeautifulSoup(text_content, 'html.parser')

        # 1. ì œëª© ì¶”ì¶œ (og:title ë©”íƒ€ íƒœê·¸ > title íƒœê·¸ ìˆœìœ¼ë¡œ ì‹œë„)
        title_tag = soup.find("meta", property="og:title")
        title = title_tag["content"] if title_tag and title_tag.get("content") else (soup.title.string if soup.title else "ì œëª© ì—†ìŒ")
        title = title.strip() if title else "ì œëª© ì—†ìŒ"

        # 2. ë³¸ë¬¸ ì¶”ì¶œ (ë‹¤ì–‘í•œ ë³¸ë¬¸ íƒœê·¸/í´ë˜ìŠ¤/ID ì‹œë„)
        body = ""
        possible_body_selectors = [
            'div.article_view',         # <-- ì—¬ê¸°ë¥¼ ê°€ì¥ ìœ„ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤!
            'div.article_head_body',    # <-- article_viewë¥¼ ê°ì‹¸ëŠ” ë¶€ëª¨ íƒœê·¸ë„ ì¶”ê°€ (ë³´í—˜ìš©)
            'div#article_content',      # ê¸°ì¡´ ë””ì§€í„¸íƒ€ì„ì¦ˆ ì¶”ì • ì„ íƒì (ë‚¨ê²¨ë‘ )
            'div.article_body_content', # ë„¤ì´ë²„ ë‰´ìŠ¤
            'div#harmonyContainer',     # ë‹¤ìŒ ë‰´ìŠ¤
            'div.article-view-content-wrapper',
            'div.news_content',
            'div.article_content',
            'div.body_content',
            'div.entry-content',
            'section.article-body',
            'div.view_page_text',
            'div[itemprop="articleBody"]',
            'article',
            'div#articleBody',
            'div.contents_area',
            'div.news_view',
            'div.view_txt',
            'div.txt_area',
            'div.article_text',
            'div.news_content_area',
            'div.cont_art',
        ]
        
        for selector in possible_body_selectors:
            body_element = None
            if '[' in selector and ']' in selector: # ì†ì„± ì„ íƒì (ì˜ˆ: div[itemprop="articleBody"])
                body_element = soup.select_one(selector)
            elif '.' in selector: # í´ë˜ìŠ¤ ì„ íƒì
                tag, class_name = selector.split('.')
                body_element = soup.find(tag, class_=class_name)
            elif '#' in selector: # ID ì„ íƒì
                tag, id_name = selector.split('#')
                body_element = soup.find(tag, id=id_name)
            else: # íƒœê·¸ ì´ë¦„ë§Œ
                body_element = soup.find(selector)
            
            if body_element:
                # ìŠ¤í¬ë¦½íŠ¸, ê´‘ê³ , ì£¼ì„ ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for s in body_element.find_all(['script', 'style', 'ins', 'iframe', 'noscript', 'img', 'figure', 'ul', 'ol', 'blockquote', 'a', 'strong', 'em']):
                    if s.get_text(strip=True).strip() == '' or s.name in ['script', 'style', 'ins', 'iframe', 'noscript']:
                        s.decompose()
                
                body = body_element.get_text(separator=' ', strip=True)
                body = re.sub(r'\s+', ' ', body).strip()

                if len(body) > 150: # ìµœì†Œ 150ì ì´ìƒì´ë©´ ìœ íš¨í•˜ë‹¤ê³  íŒë‹¨
                    break
        
        # ìµœí›„ì˜ ìˆ˜ë‹¨: ë§Œì•½ ìœ„ ì„ íƒìë“¤ë¡œ ë³¸ë¬¸ ì¶”ì¶œì— ì‹¤íŒ¨í•˜ë©´, ëª¨ë“  <p> íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ëª¨ìœ¼ê¸°
        if not body or len(body) < 150:
            all_paragraphs = soup.find_all('p')
            # ì§§ì€ p íƒœê·¸(ì˜ˆ: ì €ì‘ê¶Œ, ê¸°ì ì´ë¦„)ëŠ” ì œì™¸í•˜ëŠ” í•„í„°ë§ ê°•í™”
            valid_paragraphs = [p for p in all_paragraphs if len(p.get_text(strip=True)) > 20 and not p.find_parent(class_=['reply', 'footer', 'copyright', 'ad_unit'])]
            if valid_paragraphs:
                temp_body = ' '.join(p.get_text(separator=' ', strip=True) for p in valid_paragraphs)
                temp_body = re.sub(r'\s+', ' ', temp_body).strip()
                if len(temp_body) > 100:
                    body = temp_body
        
        if not body:
            body = "ë³¸ë¬¸ ì—†ìŒ"

        # 3. ì¶œì²˜ ì¶”ì¶œ (URLì—ì„œ ë„ë©”ì¸ ì´ë¦„ ì¶”ì¶œ ê°œì„ )
        match = re.search(r"https?://(?:www\.)?([a-zA-Z0-9-]+\.(?:co\.kr|com|net|org|kr|io|dev|ai|app|info|biz|tv|news|me|blog|cc|xyz)[^/]*)/?", url)
        if match and match.group(1):
            domain_full = match.group(1)
            if '.co.kr' in domain_full:
                source = domain_full.split('.')[0]
            else:
                source = domain_full.split('.')[-2]
        else:
            source = "ì¶œì²˜ ë¶ˆëª…"
        
        source = source.replace('-', '').strip()
        if not source: source = "ì¶œì²˜ ë¶ˆëª…"

        return title, body, source

    except requests.exceptions.RequestException as e:
        st.error(f"ì›¹ ìš”ì²­ ì˜¤ë¥˜: {e}. URLì„ ë‹¤ì‹œ í™•ì¸í•˜ê±°ë‚˜ ì¸í„°ë„· ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return "", "", ""
    except Exception as e:
        st.warning(f"ë§í¬ ë¶„ì„ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}. ì¼ë¶€ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return "ì œëª© ì—†ìŒ", "ë³¸ë¬¸ ì—†ìŒ", "ì¶œì²˜ ë¶ˆëª…"


# --- Streamlit ì•± UI ---
st.header("ğŸ“° ë‚šì‹œì„± ë‰´ìŠ¤ íŒë³„ê¸°")
st.markdown("""
    ë‰´ìŠ¤ ì œëª©, ë³¸ë¬¸ ë˜ëŠ” ê¸°ì‚¬ ë§í¬ë¥¼ ì…ë ¥í•˜ë©´
    ìì²´ í•™ìŠµí•œ AIê°€ ë‰´ìŠ¤ì˜ ë‚šì‹œì„± ì •ë„ë¥¼ ë¶„ì„í•´ë“œë¦½ë‹ˆë‹¤. ê°€ì§œ ë‰´ìŠ¤ íŒë³„ê¸°ê°€ ì•„ë‹Œ ë‚šì‹œì„± ë‰´ìŠ¤ íŒë³„ê¸°ë¡œ ë³¸ë¬¸ê³¼ ë‹¤ë¥´ê±°ë‚˜ ê³¼ì¥, ê±°ì§“ëœ ê¸°ì‚¬ë¥¼ íŒë³„í•˜ë©° íŒë³„ ê²°ê³¼ê°€ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")


st.markdown(
    """
    <hr style='margin-top: 20px; margin-bottom: 10px; border: none; height: 1px; background-color: #ccc;' />
    """,
    unsafe_allow_html=True
) # ì—¬ê¸°ì— ì²« ë²ˆì§¸ êµ¬ë¶„ì„ ì´ ìˆìŠµë‹ˆë‹¤.

# ğŸ¨ ì»¤ìŠ¤í…€ CSS ì£¼ì… START
st.markdown("""
<style>
/* íŒë³„í•˜ê¸° ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
/* primaryColorë¥¼ ë”°ë¥´ë„ë¡ type="primary"ë¥¼ ì‚¬ìš©í•˜ë©´ì„œ í°íŠ¸ í¬ê¸° ì¡°ì ˆ */
/* Streamlit 1.x ë²„ì „ì—ì„œëŠ” data-testidê°€ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì‹¤ì œ HTML êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì—¬ ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. */
/* ì¼ë°˜ì ì¸ primary ë²„íŠ¼ì˜ ì„ íƒì */
button[data-testid="stButton"] {
    background-color: #4979ea; /* primaryColorì— ì´ë¯¸ ì„¤ì •í–ˆì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ ì§€ì • */
    color: white;
    font-size: 1.5em; /* í°íŠ¸ í¬ê¸° í‚¤ì›€ (1.5ë°°) */
    padding: 15px 30px; /* íŒ¨ë”©ì„ ëŠ˜ë ¤ ë²„íŠ¼ í¬ê¸°ë¥¼ í‚¤ì›€ */
    border-radius: 8px; /* ëª¨ì„œë¦¬ë¥¼ ë‘¥ê¸€ê²Œ */
    border: none;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2); /* ê·¸ë¦¼ì íš¨ê³¼ ì¶”ê°€ */
    transition: background-color 0.3s ease; /* í˜¸ë²„ ì‹œ ë¶€ë“œëŸ¬ìš´ ì „í™˜ */
}

button[data-testid="stButton"]:hover {
    background-color: #3b60c7; /* í˜¸ë²„ ì‹œ ì•½ê°„ ë” ì§„í•œ ìƒ‰ìƒ */
}

/* ì„ íƒ ë°©ì‹ ë¼ë””ì˜¤ ë²„íŠ¼ì˜ ì›í˜• ì²´í¬ ìƒ‰ìƒ */
/* Streamlit ë¼ë””ì˜¤ ë²„íŠ¼ì˜ HTML êµ¬ì¡°ëŠ” ë³µì¡í•˜ë¯€ë¡œ, ê°€ì¥ ì•ˆì •ì ì¸ ì„ íƒìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. */
div.stRadio > label > div[data-testid="stCheck"] > div {
    border-color: #4979ea !important; /* ì²´í¬ë°•ìŠ¤/ë¼ë””ì˜¤ ë²„íŠ¼ í…Œë‘ë¦¬ ìƒ‰ìƒ */
}
div.stRadio > label > div[data-testid="stCheck"] > div > svg {
    fill: #4979ea !important; /* ì²´í¬ í‘œì‹œ ë˜ëŠ” ì›í˜• ë‚´ë¶€ ìƒ‰ìƒ */
}

/* ì„ íƒ ë°©ì‹ ë¼ë””ì˜¤ ë²„íŠ¼ì˜ í…ìŠ¤íŠ¸ ìƒ‰ìƒ (í™œì„± ìƒíƒœ) */
div.stRadio > label.st-selected > div > p {
    color: #4979ea !important; /* ì„ íƒëœ ë¼ë””ì˜¤ ë²„íŠ¼ í…ìŠ¤íŠ¸ ìƒ‰ìƒ */
    font-weight: bold; /* ì„ íƒëœ í…ìŠ¤íŠ¸ êµµê²Œ */
}

/* ì„ íƒ ë°©ì‹ ë¼ë””ì˜¤ ë²„íŠ¼ì˜ í…ìŠ¤íŠ¸ í¬ê¸° (í•„ìš” ì‹œ) */
div.stRadio > label > div > p {
    font-size: 1em;
} */


/* ë‹¤ë¥¸ ì…ë ¥ í•„ë“œ í…ìŠ¤íŠ¸ í¬ê¸° (ì„ íƒ ì‚¬í•­) */
/* .stTextArea textarea, .stTextInput input {
    font-size: 1em;
} */

/* ì„¹ì…˜ ì œëª©(subheader) ìƒ‰ìƒ ë³€ê²½ (ì„ íƒ ì‚¬í•­) */
h2 {
    color: #4979ea; /* ì„¹ì…˜ ì œëª© ìƒ‰ìƒ */
}

</style>
""", unsafe_allow_html=True)
# ğŸ¨ ì»¤ìŠ¤í…€ CSS ì£¼ì… END


# ğŸ£ ì‚¬ì´ë“œë°” ì¶”ê°€ START
with st.sidebar:
    st.header("âš™ï¸ ì‚¬ì´íŠ¸ ì •ë³´")
    st.write("ì´ ì‚¬ì´íŠ¸ëŠ” ë§¤íƒ„ê³ ë“±í•™êµ í•™ìƒë“¤ì´ ìê·¹ì ì¸ ì œëª©ì´ë‚˜ ë‚šì‹œì„± ê¸°ì‚¬ì— ì‰½ê²Œ í˜„í˜¹ë˜ì§€ ì•Šê³ , ì˜¬ë°”ë¥¸ ì •ë³´ íŒë‹¨ ëŠ¥ë ¥ì„ ê¸°ë¥¼ ìˆ˜ ìˆë„ë¡ ì œì‘ëœ ì‚¬ì´íŠ¸ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë‚šì‹œì„± ì—¬ë¶€ë¥¼ í…ìŠ¤íŠ¸ ë¶„ì„ì„ í†µí•´ íŒë³„í•©ë‹ˆë‹¤.")
    st.write("**ê°œë°œì:** GOMHAN") # ì—¬ê¸°ì— ê·€í•˜ì˜ ì •ë³´ ì¶”ê°€
    st.write("**ë²„ì „:** 1.0.1")
    st.markdown("---")
    st.subheader("âœ”ï¸ ì¶”ì²œ ê²€ì‚¬ ë°©ì‹")
    st.write("""
    **ë‹¤ìŒ ìˆœì„œì— ë”°ë¼ ê²€ì‚¬ ì •í™•ë„ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤**
    1. ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ ì…ë ¥
    2. ì œëª© + ë³¸ë¬¸ ì…ë ¥
    3. ì œëª©ë§Œ ì…ë ¥
    """)
    st.markdown("---")
    st.subheader("â“ ì‚¬ìš©ë²•")
    st.write("""
    1.  **ê²€ì‚¬ ë°©ì‹ ì„ íƒ:** ì œëª©ë§Œ ì…ë ¥, ì œëª©+ë³¸ë¬¸ ì…ë ¥, ë§í¬ ì…ë ¥ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤ë‹¤.
    2.  **ë‚´ìš© ì…ë ¥:** ì„ íƒí•œ ë°©ì‹ì— ë§ê²Œ í…ìŠ¤íŠ¸ë‚˜ ë§í¬ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.
    3.  **'íŒë³„í•˜ê¸°' ë²„íŠ¼ í´ë¦­:** AIëª¨ë¸ì´ ì…ë ¥ëœ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """)
    st.markdown("---")
    st.caption("""
    ë³¸ ì‚¬ì´íŠ¸ëŠ” ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ì˜ ì¬ì›ìœ¼ë¡œ í•œêµ­ì§€ëŠ¥ì •ë³´ì‚¬íšŒì§„í¥ì›ì˜ ì§€ì›ì„ ë°›ì•„ êµ¬ì¶•ëœ "ë‚šì‹œì„± ê¸°ì‚¬ íƒì§€ ë°ì´í„°"ì„ í™œìš©í•˜ì—¬ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ì‚¬ì´íŠ¸ì— í™œìš©ëœ ë°ì´í„°ëŠ” AI í—ˆë¸Œì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    st.markdown(
    """
    <div style="display: flex; justify-content: space-between; align-items: center;">
        <span>ë¬¸ì˜ : JH.Moon213@gmail.com</span>
        <img src="https://i.imgur.com/VWthizR.png" width="40" style="margin-left: 10px;" />
    </div>
    """,
    unsafe_allow_html=True
    )

   

# ğŸ£ ì‚¬ì´ë“œë°” ì¶”ê°€ END

# ğŸ“ ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ START
col1, col2 = st.columns([1, 2]) # UI ì»¬ëŸ¼ ë¶„í• 

with col1: # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì— ê²€ì‚¬ ë°©ì‹ ì„ íƒ ë°°ì¹˜
    st.subheader("â‘  ê²€ì‚¬ ë°©ì‹ ì„ íƒ")
    # ê¸°ì¡´ st.radio ì½”ë“œë¥¼ ì—¬ê¸°ì— ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.
    check_method = st.radio(
        "ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë‰´ìŠ¤ ë‚´ìš©ì„ ê²€ì‚¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? ğŸ‘‡",
        ("â‘  ì œëª©ë§Œ ì…ë ¥", "â‘¡ ì œëª© + ë³¸ë¬¸ ì…ë ¥", "â‘¢ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ ì…ë ¥"),
        index=0, # ê¸°ë³¸ê°’ì€ "ì œëª©ë§Œ ì…ë ¥"
        key="check_method_radio"
    )
    st.markdown("<br>", unsafe_allow_html=True)

with col2: # ë‘ ë²ˆì§¸ ì»¬ëŸ¼ì— ì…ë ¥ í•„ë“œ ë°°ì¹˜
    st.subheader("â‘¡ ì •ë³´ ì…ë ¥")
    # ê¸°ì¡´ ì…ë ¥ í•„ë“œ ì½”ë“œë¥¼ ì—¬ê¸°ì— ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.
    title_input = ""
    body_input = ""
    link_input = ""
    text_to_analyze = ""
    accuracy_hint = ""

    if check_method == "â‘  ì œëª©ë§Œ ì…ë ¥":
        title_input = st.text_area(
            "íŒë³„í•˜ê³  ì‹¶ì€ ë‰´ìŠ¤ ì œëª© ë˜ëŠ” ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: 'ì´ê²ƒë§Œ ì•Œë©´ ë‹¹ì‹ ë„ ë¶€ì! ë¹„ë°€ ê³µê°œ'",
            height=100,
            key="title_only_input"
        )
        accuracy_hint = "ì •í™•ë„: ë‚®ìŒ (ì œëª©ë§Œ ì‚¬ìš©)"
        text_to_analyze = title_input

    elif check_method == "â‘¡ ì œëª© + ë³¸ë¬¸ ì…ë ¥":
        title_input = st.text_input(
            "ë‰´ìŠ¤ ì œëª©ì„ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: 'ì´ê²ƒë§Œ ì•Œë©´ ë‹¹ì‹ ë„ ë¶€ì! ë¹„ë°€ ê³µê°œ'",
            key="title_and_body_title_input"
        )
        body_input = st.text_area(
            "ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš©ì„ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.",
            height=200,
            key="title_and_body_body_input"
        )
        accuracy_hint = "ë³´í†µ: ë†’ìŒ (ì œëª© + ë³¸ë¬¸ ì‚¬ìš©)"

    elif check_method == "â‘¢ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ ì…ë ¥":
        link_input = st.text_input(
            "ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬(URL)ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: https://news.naver.com/main/read.naver?mode=LSD&mid=shm&sid1=105&oid=001&aid=0012345678",
            key="link_input"
        )
        accuracy_hint = "ì •í™•ë„: ë†’ìŒ (ì œëª© + ë³¸ë¬¸ + ì¶œì²˜ ì‚¬ìš©)"

st.markdown("---")

# âœ… íŒë³„í•˜ê¸° ë²„íŠ¼ START
# ê¸°ì¡´ 'if st.button("íŒë³„í•˜ê¸°", key="predict_button"):' ë¼ì¸ì„ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•©ë‹ˆë‹¤.
col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 1])
with col_btn2:
    if st.button("âœ¨ íŒë³„í•˜ê¸° âœ¨", key="predict_button", use_container_width=True, type="primary"):
        # `type="primary"`ë¥¼ ì‚¬ìš©í•˜ë©´ `config.toml`ì˜ `primaryColor`ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
        # `config.toml`ë¡œ ìƒ‰ìƒì„ ë³€ê²½í–ˆìœ¼ë¯€ë¡œ, CSSì—ì„œëŠ” í¬ê¸°, ê·¸ë¦¼ì ë“±ë§Œ ì œì–´í•˜ë©´ ë©ë‹ˆë‹¤.
        text_to_analyze = ""
        accuracy_hint = ""
        
        # ì…ë ¥ê°’ ê²€ì¦ ë° ë¶„ì„í•  í…ìŠ¤íŠ¸ ì¤€ë¹„
        if check_method == "â‘  ì œëª©ë§Œ ì…ë ¥":
            if not title_input.strip():
                st.warning("ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                render_footer()
                st.stop()
            text_to_analyze = title_input
            accuracy_hint = "ì •í™•ë„: ë‚®ìŒ (ì œëª©ë§Œ ì‚¬ìš©)"  # âœ… ì¶”ê°€

        elif check_method == "â‘¡ ì œëª© + ë³¸ë¬¸ ì…ë ¥":
            if not title_input.strip() and not body_input.strip():
                st.warning("ì œëª©ê³¼ ë³¸ë¬¸ ì¤‘ í•˜ë‚˜ë¼ë„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                render_footer()
                st.stop()
            
            if title_input.strip() and not body_input.strip():
                text_to_analyze = title_input
                accuracy_hint = "ì •í™•ë„: ë‚®ìŒ (ì œëª©ë§Œ ì‚¬ìš©)"
            elif not title_input.strip() and body_input.strip():
                text_to_analyze = body_input
                accuracy_hint = "ì •í™•ë„: ë³´í†µ (ë³¸ë¬¸ë§Œ ì‚¬ìš©)"
            else:
                text_to_analyze = title_input + " " + body_input
                accuracy_hint = "ì •í™•ë„: ë†’ìŒ (ì œëª© + ë³¸ë¬¸ ì‚¬ìš©)"

        elif check_method == "â‘¢ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ ì…ë ¥":
            if not link_input.strip():
                st.warning("ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                render_footer()
                st.stop()
            # êµ¬ê¸€
            
            if "google.com" in link_input and ("read" in link_input or "/amp/" in link_input):
                st.warning("âŒ Google ë‰´ìŠ¤ ë§í¬ëŠ” ì™¸ë¶€ ê¸°ì‚¬ì˜ ì¤‘ê°„ ë§¤ê°œì²´ ì—­í• ì„ í•˜ë¯€ë¡œ ì‹¤ì œ ë‰´ìŠ¤ ë‚´ìš©ì„ ì§ì ‘ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.info("ğŸ”— ì•„ë˜ ë°©ë²•ì„ ë”°ë¼ì£¼ì„¸ìš”:\n1. Google ë‰´ìŠ¤ ë§í¬ë¥¼ ì¸í„°ë„·ì—ì„œ ì§ì ‘ë“¤ì–´ê°„ë‹¤.\n2. ìƒë‹¨ ì£¼ì†Œì°½ì— í‘œì‹œëœ **ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬**ë¥¼ ë³µì‚¬í•œë‹¤.\n3. ë³µì‚¬í•œ ë§í¬ë¥¼ ë‹¤ì‹œ ì´ê³³ì— ë¶™ì—¬ë„£ëŠ”ë‹¤.")
                render_footer()
                st.stop()
            
            with st.spinner('ğŸ”— ë§í¬ì—ì„œ ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ ì¤‘... (ìµœëŒ€ 10ì´ˆ)'):
                title_extracted, body_extracted, source_extracted = extract_info_from_url(link_input)
            # í…ìŠ¤íŠ¸ ê¹¨ì§ ê°ì§€ í•¨ìˆ˜ ì •ì˜
            def is_garbled(text):
                if not text or len(text.strip()) == 0:
                    return True
                korean = re.findall(r"[ê°€-í£]", text)
                english = re.findall(r"[a-zA-Z]", text)
                digits = re.findall(r"[0-9]", text)
                valid_ratio = (len(korean) + len(english) + len(digits)) / len(text)
                return valid_ratio < 0.2

            # ê¹¨ì§ ì—¬ë¶€ ê²€ì‚¬
            if is_garbled(title_extracted) or is_garbled(body_extracted):
                st.warning("âŒ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì„ ì •ìƒì ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                st.info("ğŸ‘‰ **â€˜ì œëª©ë§Œ ì…ë ¥â€™ ë˜ëŠ” â€˜ì œëª© + ë³¸ë¬¸ ì…ë ¥â€™ ê¸°ëŠ¥ì„ ì´ìš©í•´ ì£¼ì„¸ìš”.**")
                render_footer()
                st.stop()
            
            if not title_extracted and not body_extracted:
                st.error("âŒ ë§í¬ì—ì„œ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë§í¬ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ë§í¬ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”.")
                render_footer()
                st.stop()
            elif title_extracted == "ì œëª© ì—†ìŒ" and body_extracted == "ë³¸ë¬¸ ì—†ìŒ":
                st.error("âš ï¸ ë§í¬ì—ì„œ ìœ íš¨í•œ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ë§ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                render_footer()
                st.stop()
            else:
                text_to_analyze = f"{title_extracted} {body_extracted} {source_extracted}"
                st.success(f"âœ… **ì¶”ì¶œëœ ì œëª©:** {title_extracted if title_extracted != 'ì œëª© ì—†ìŒ' else 'ì œëª© ì—†ìŒ'}")
                st.info(f"**ì¶”ì¶œëœ ì¶œì²˜:** {source_extracted}")
                if body_extracted != "ë³¸ë¬¸ ì—†ìŒ":
                    with st.expander("ğŸ“ ì¶”ì¶œëœ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°"): # ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°ë¥¼ Expanderë¡œ ê°ì‹¸ê¸°
                        st.write(body_extracted[:500] + "..." if len(body_extracted) > 500 else body_extracted)
                else:
                    st.warning("âš ï¸ ë³¸ë¬¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì œëª©ê³¼ ì¶œì²˜ ì •ë³´ë§Œìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
                if title_extracted and body_extracted:
                        accuracy_hint = "ì •í™•ë„: ë†’ìŒ (ì œëª© + ë³¸ë¬¸ ì‚¬ìš©)"
                elif title_extracted and not body_extracted:
                        accuracy_hint = "ì •í™•ë„: ë‚®ìŒ (ì œëª©ë§Œ ì‚¬ìš©)"
                elif not title_extracted and body_extracted:
                        accuracy_hint = "ì •í™•ë„: ë³´í†µ (ë³¸ë¬¸ë§Œ ì‚¬ìš©)"
                else:
                        accuracy_hint = "ì •í™•ë„: ë¶ˆëª…í™• (ì •ë³´ ë¶€ì¡±)"

            # ìµœì¢… ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆë‹¤ë©´ ì˜¤ë¥˜ ì²˜ë¦¬
            if not text_to_analyze.strip():
                st.warning("ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì¤€ë¹„í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                render_footer()
                st.stop()
if not text_to_analyze.strip():
    st.warning("âŒ ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ì–´ ì˜ˆì¸¡ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    render_footer()
    st.stop()
# --- ëª¨ë¸ ì˜ˆì¸¡ ---
with st.spinner("ğŸ§  ëª¨ë¸ì´ ë‚šì‹œì„± ì—¬ë¶€ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
    X_vec = vectorizer.transform([text_to_analyze])
    
    probabilities = model.predict_proba(X_vec)[0]
    
    try:
        clickbait_class_index = list(model.classes_).index(1)
        prob_clickbait = probabilities[clickbait_class_index]
        percent_clickbait = round(prob_clickbait * 100, 2)
    except ValueError:
        st.error("ì˜¤ë¥˜: ëª¨ë¸ í´ë˜ìŠ¤ì— ë‚šì‹œì„±/ì •ìƒ ë¼ë²¨(0 ë˜ëŠ” 1)ì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ëª¨ë¸ í•™ìŠµì„ í™•ì¸í•˜ì„¸ìš”.")
        render_footer()
        st.stop()

    predicted_label = model.predict(X_vec)[0]

    # âœ… ì´ ì•ˆìª½ì—ì„œ percent_clickbait ì‚¬ìš© ê°€ëŠ¥!
    st.markdown("---")
    st.subheader("ğŸ“Š íŒë³„ ê²°ê³¼")
    st.markdown("<br>", unsafe_allow_html=True)

    if percent_clickbait > 60:
        st.markdown(
            f"<p style='font-size:17px;'>ğŸš¨ ì´ ê¸°ì‚¬ëŠ” <strong>ë‚šì‹œì„± ë‰´ìŠ¤</strong>ì¼ í™•ë¥ ì´ <strong>{percent_clickbait}%</strong> ì…ë‹ˆë‹¤!</p>",
            unsafe_allow_html=True
        )
        st.error("â— **ë†’ì€ í™•ë¥ ë¡œ ë…ìì˜ í´ë¦­ì„ ìœ ë„í•˜ëŠ” ìš”ì†Œë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.**")
        if percent_clickbait > 80:
            st.caption("ì£¼ì˜! ìê·¹ì ì¸ í‘œí˜„ì´ë‚˜ ê³¼ì¥ëœ ë‚´ìš©ì´ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif percent_clickbait > 70:
            st.caption("ë‚šì‹œì„± ê°€ëŠ¥ì„±ì´ ë†’ì€ ê¸°ì‚¬ì…ë‹ˆë‹¤. ë‚´ìš©ì„ ì£¼ì˜ ê¹Šê²Œ í™•ì¸í•´ë³´ì„¸ìš”.")
        else:
            st.caption("ë‚šì‹œì„±ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆìœ¼ë‚˜ í™•ë¥ ì€ ì¤‘ê°„ ì •ë„ì…ë‹ˆë‹¤.")
    else:
        st.markdown(
            f"## âœ… ì´ ê¸°ì‚¬ëŠ” **ì •ìƒ ë‰´ìŠ¤**ì¼ í™•ë¥ ì´ `{100 - percent_clickbait}%` ì…ë‹ˆë‹¤.",
            unsafe_allow_html=True
        )
        st.success("ğŸ‘ **ë‚šì‹œì„± íŠ¹ì§•ì´ ê±°ì˜ ì—†ëŠ” ì¼ë°˜ì ì¸ ë‰´ìŠ¤ì…ë‹ˆë‹¤.**")
        if percent_clickbait < 30:
            st.caption("ì•ˆì‹¬í•˜ê³  ì½ìœ¼ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤.")
        elif percent_clickbait <= 60:
            st.caption("ì •ìƒ ë‰´ìŠ¤ë¡œ ë¶„ë¥˜ë˜ì—ˆì§€ë§Œ, ë‹¤ì†Œ ìê·¹ì ì¸ í‘œí˜„ì´ í¬í•¨ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.")

    st.info(accuracy_hint)

    # âœ… Google Sheets ë¡œê·¸ ì €ì¥
    log_to_google_sheets(
        method=check_method,
        input_text=text_to_analyze,
        result="Clickbait" if predicted_label == 1 else "Normal",
        score=percent_clickbait
    )

render_footer()
